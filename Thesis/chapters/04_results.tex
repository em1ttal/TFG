\chapter{Results and Discussion}

Our experiments revealed significant insights into the behavior of TabNet under unlearning conditions, particularly highlighting the trade-off between utility preservation and forgetting effectiveness.

\section{Sensitivity of TabNet}
One of the most critical findings was the extreme sensitivity of TabNet to weight perturbations. We found that TabNet requires "ultra-conservative" noise levels (e.g., $\sigma \approx 0.01$ for Gaussian noise) to avoid catastrophic failure. Higher noise levels ($>0.1$) rapidly destroyed the model's ability to classify even the retained data, reducing accuracy to random guessing.

\section{Performance of Strategies}

\subsection{Forgetting Effectiveness}
Contrary to our initial expectations, all noise-based strategies struggled to achieve the target forget accuracy of 25\% (random guessing). The Forget Accuracy remained high ($>90\%$) for all methods, indicating that the model retained significant information about the forget set.

\begin{itemize}
    \item \textbf{Gradient-based Unlearning}: This was the top-performing strategy overall. It achieved the lowest Forget Accuracy (~96.6\%) among all methods, though still far from the target. It effectively identified the most relevant weights but was limited by the need to preserve utility.
    \item \textbf{Noise Injection Methods (Gaussian, Laplacian, Adaptive)}: These methods proved less effective at forgetting, with Forget Accuracy often exceeding 97\%. The perturbations, while sufficient to slightly degrade performance, were not targeted enough to erase the specific decision boundaries of the forget class without damaging the rest of the model.
\end{itemize}

\subsection{Utility Preservation}
All strategies excelled at utility preservation. Retain Accuracy and Test Accuracy consistently remained high ($>96\%$), closely matching the baseline model. This suggests that the "ultra-conservative" noise levels successfully protected the model's general knowledge, even if they were insufficient for complete unlearning.

\subsection{Privacy Guarantee (MIA)}
We evaluated the Membership Inference Attack (MIA) AUC to measure privacy leakage. A value of 0.5 indicates perfect privacy (random guessing by the attacker).
\begin{itemize}
    \item \textbf{Gradient-based}: Achieved an MIA AUC of ~0.475, closest to the target of 0.5.
    \item \textbf{Adaptive Noise}: Also performed well with an AUC of ~0.499.
    \item \textbf{Gaussian/Laplacian}: Showed slightly higher leakage (AUC > 0.50), indicating that the noise injection left some distinguishable traces.
\end{itemize}

\section{Strategy Ranking}
Based on our Balance Score (Retain + Test - Forget), the strategies were ranked as follows:
\begin{enumerate}
    \item \textbf{Gradient-based Unlearning} (Score: ~0.982)
    \item \textbf{Gaussian Noise} (Score: ~0.978)
    \item \textbf{Layer-wise Noise} (Score: ~0.974)
\end{enumerate}

While Gradient-based unlearning provided the best trade-off, the results highlight the inherent difficulty of unlearning in sparse architectures like TabNet using simple noise or gradient mechanisms alone.
