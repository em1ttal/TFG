\chapter{Experimental Setup}

To validate our methodology, we conducted a comprehensive evaluation. Our experiments follow the "dataset evolution" described in the methodology, starting with visual validation on spirals and proceeding to statistical validation on the Adult Income dataset.

\section{The Baseline: Retraining from Scratch}
For each Forget Request, we trained a fresh TabNet model solely on the retain set $D_r$. This serves as our "Gold Standard." A perfect unlearning strategy would match this baseline's performance exactly:
\begin{itemize}
    \item \textbf{Retain Accuracy}: Should match the baseline (high).
    \item \textbf{Forget Accuracy}: Should match the baseline (typically random chance).
    \item \textbf{Test Accuracy}: Should match the baseline (high generalization).
\end{itemize}

\section{Datasets}

\subsection{Spiral Dataset (Phase 1)}
We generated a dataset with $N=1200$ samples across 4 classes.
\begin{itemize}
    \item \textbf{Setup}: We varied the noise parameters to create "short and fat" vs "long and thin" spirals.
    \item \textbf{Selection}: We settled on a noise standard deviation of 0.5 to ensure the classes were distinct but had enough boundary overlap to make the classification non-trivial.
    \item \textbf{Objective}: Visual confirmation of decision boundary shifts.
\end{itemize}

\subsection{Adult Income Dataset (Phase 2)}
We utilized the standard Adult Income dataset ($N \approx 48,000$ after cleaning).
\begin{itemize}
    \item \textbf{Preprocessing}: 
    \begin{itemize}
        \item \textbf{Missing Values}: Handled '?' placeholders (both with and without whitespace) by creating an explicit "Unknown" category to preserve information.
        \item \textbf{Feature Engineering}: We simplified education levels (16 $\rightarrow$ 5 categories) and created binary flags for key indicators like \textit{is\_married}, \textit{has\_capital}, and \textit{is\_us\_native}.
        \item \textbf{Normalization}: Numerical features were standardized, and categorical features were label-encoded.
    \end{itemize}
    \item \textbf{Task}: Binary classification (Income $>50K$ vs $\le 50K$).
    \item \textbf{Objective}: Statistical confirmation of privacy/utility trade-offs in a high-dimensional space.
\end{itemize}

\subsection{Training Hyperparameters}
We optimized the training process to ensure stable convergence:
\begin{itemize}
    \item \textbf{Epochs}: 300 (increased to allow full convergence)
    \item \textbf{Patience}: 50 epochs (for early stopping)
    \item \textbf{Learning Rate}: $1e-2$ with a StepLR scheduler (step size 30, gamma 0.95)
    \item \textbf{Batch Size}: 1024 (with virtual batch size 128)
\end{itemize}

\section{Evaluation Metrics}
We assess performance using four key metrics:
\begin{enumerate}
    \item \textbf{Retain Accuracy}: The accuracy on the data we intend to keep. We want this to remain high.
    \item \textbf{Forget Accuracy}: The accuracy on the data we want to remove. We want this to drop significantly to random chance levels (25\% for Spirals, 50\% for Adult).
    \item \textbf{Test Accuracy}: The accuracy on unseen data. This ensures the model hasn't just broken its decision boundaries but remains a useful classifier.
    \item \textbf{MIA AUC}: The Area Under the Curve for Membership Inference Attacks. A value close to 0.5 indicates successful privacy preservation (random guessing by the attacker), while 1.0 indicates complete information leakage.
\end{enumerate}

To rank the strategies, we also define a \textbf{Balance Score}:
\[
\text{Score} = \text{Retain Acc} + \text{Test Acc} - \text{Forget Acc}
\]
A higher score indicates a better trade-off.
