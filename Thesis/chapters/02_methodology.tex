\chapter{Methodology}

To rigorously evaluate machine unlearning strategies, we designed an experimental framework that evolves from controlled synthetic environments to complex real-world scenarios. This "evolutionary" approach allows us to first validate our hypotheses on interpretable data before applying them to high-dimensional "proper" data.

\section{Model Architecture: TabNet}

For our classification task, we employ \textbf{TabNet} (Attentive Interpretable Tabular Learning) \cite{TabNet}. While decision trees (like XGBoost) have traditionally dominated tabular data tasks, TabNet brings the benefits of deep learning—such as end-to-end learning and representation learning—to tabular domains.

\subsection{Why TabNet?}
TabNet uses a sequential attention mechanism to choose which features to reason from at each decision step. This enables interpretability (we can see which features matter) and efficient learning. Unlike standard Multi-Layer Perceptrons (MLPs), TabNet creates a decision-tree-like logic within a neural network.

\subsection{Entmax vs. Sparsemax}
A key component of TabNet is its masking mechanism. We experimented with different attention mappings. While \textbf{Softmax} provides a dense distribution (all features get some weight), \textbf{Sparsemax} forces many weights to exactly zero, creating sparse selections. We ultimately utilized \textbf{Entmax} (specifically 1.5-entmax), which offers a middle ground: it is sparse like Sparsemax but smoother, allowing for better gradient flow during the unlearning process.

\section{Dataset Evolution Strategy}

A core part of our methodology is the progression of datasets. We did not jump straight to complex data; instead, we followed a staged approach.

\subsection{Phase 1: Synthetic Spirals (The "Lab" Environment)}
We first generated a synthetic 2D non-linear dataset known as the "Spiral Dataset."
\begin{itemize}
    \item \textbf{Why?}: It is visually interpretable. We can plot the decision boundaries in 2D and instantly see if the unlearning strategy has "broken" the model or successfully erased a specific class region.
    \item \textbf{Variations}: During development, we experimented with different spiral shapes ("short and fat" vs. "long and thin") by adjusting the noise standard deviation and radius parameters. This helped us tune the TabNet hyperparameters to ensure it could actually learn these non-linear patterns before we attempted to unlearn them.
\end{itemize}

\subsection{Phase 2: Adult Income (The "Real-World" Environment)}
Once the unlearning strategies were validated on spirals, we evolved to the \textbf{Adult Income dataset}.
\begin{itemize}
    \item \textbf{Why?}: This represents "proper" data—real-world census records used to predict income ($>50K$ vs. $\le 50K$).
    \item \textbf{Challenges}: Unlike the clean 2D spirals, this dataset features mixed data types (categorical and numerical), class imbalance, and higher dimensionality. This tests whether our noise-based unlearning strategies can scale beyond toy problems.
\end{itemize}

\section{Unlearning Framework}

We define "Unlearning" as the process of updating a trained model $\mathcal{M}$ to remove the knowledge associated with a specific subset of data $D_f$ (the forget set), while retaining knowledge of the remaining data $D_r$ (the retain set).

We simulate three distinct "Forget Requests" to test different unlearning scenarios:
\begin{enumerate}
    \item \textbf{FR1: Remove Entire Class}: We ask the model to forget everything about a specific class (e.g., Class 2 in Spirals).
    \item \textbf{FR2: Remove Partial Class}: We remove a random 40\% subset of a class (mimicking "right to be forgotten" for specific users).
    \item \textbf{FR3: Remove Multi-Class Subset}: We remove a random 40\% subset of multiple classes.
\end{enumerate}

\section{Noise-Based Unlearning Strategies}

We implement five strategies that inject carefully calibrated noise into the model's weights. The intuition is that by perturbing the weights, we can disrupt the specific patterns learned for the forget set without destroying the global model structure.

\subsection{Strategy 1: Gaussian Noise Injection}
We add random noise drawn from a normal distribution to the model's parameters $\theta$. To avoid destroying the model completely, we scale the noise by the standard deviation of each parameter:
\[
\theta' = \theta + \mathcal{N}(0, \sigma^2) \cdot \text{std}(\theta)
\]
We use a noise scale $\sigma = 0.01$, which we found experimentally to be the "sweet spot" for TabNet \cite{Guo2020}.

\subsection{Strategy 2: Laplacian Noise Injection}
Laplacian noise is often used in differential privacy because it has a heavier tail than Gaussian noise.
\[
\theta' = \theta + \text{Laplace}(0, b) \cdot \text{std}(\theta)
\]
This allows for occasional larger perturbations, potentially helping the model escape the local minima associated with the forget data \cite{Sekhari2021}.

\subsection{Strategy 3: Adaptive Noise Injection}
Instead of uniform noise, we scale the noise based on the magnitude of the weights themselves.
\[
\theta' = \theta + \mathcal{N}(0, \sigma^2) \cdot |\theta|
\]
The intuition is that larger weights encode more "knowledge," so they should receive proportionally more noise. This is inspired by saliency-based methods like SalUn \cite{Fan2024}.

\subsection{Strategy 4: Layer-wise Noise Injection}
Deep networks process information hierarchically. We apply progressive noise scaling, where later layers (decision makers) receive up to $2\times$ more noise than early layers (feature extractors).

\subsection{Strategy 5: Gradient-Based Unlearning}
This is the most targeted strategy. Instead of random noise, we perform \textbf{Gradient Ascent} on the forget set \cite{Neel2021}.
\[
\theta \leftarrow \theta + \eta \nabla_\theta \mathcal{L}(D_f)
\]
We perform this for a few steps with a very small learning rate to gently push the model's weights away from the configuration that predicts the forget set well.
