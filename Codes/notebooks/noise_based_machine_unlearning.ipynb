{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Noise-Based Machine Unlearning Methods for Tabular Data\n",
        "\n",
        "## ðŸ“š What is Machine Unlearning?\n",
        "\n",
        "Machine unlearning is the process of making a trained model \"forget\" specific data points, as if they were never part of the training set. This is crucial for:\n",
        "\n",
        "- **Privacy Compliance**: GDPR's \"right to be forgotten\" requires deletion of user data\n",
        "- **Data Corrections**: Removing mislabeled or corrupted training samples\n",
        "- **Bias Mitigation**: Eliminating problematic data that causes unfair predictions\n",
        "- **Security**: Preventing memorization of sensitive information\n",
        "\n",
        "**The Challenge**: Simply deleting data isn't enoughâ€”the model has already learned from it! Retraining from scratch is expensive and often infeasible for large models.\n",
        "\n",
        "**The Solution**: Approximate unlearning methods that are much faster than retraining.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ¯ Noise-Based Unlearning: Core Concept\n",
        "\n",
        "**Key Insight**: Neural networks store learned information in their parameters (weights and biases). By carefully adding noise to these parameters, we can disrupt the specific patterns associated with the forget set while preserving general knowledge.\n",
        "\n",
        "**Why Noise Works**:\n",
        "- Small random perturbations break precise memorization\n",
        "- Model becomes less confident about specific forget set samples\n",
        "- General patterns (shared with retain set) are more robust to noise\n",
        "- Provides plausible deniability about forget set data\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¬ Methods Implemented:\n",
        "\n",
        "### 1. **Gaussian Noise Injection** \n",
        "   - **Approach**: Add N(0, ÏƒÂ²) noise uniformly to all parameters\n",
        "   - **Pros**: Simple, easy to tune, well-understood statistics\n",
        "   - **Cons**: Treats all parameters equally (may be inefficient)\n",
        "   - **Use case**: Baseline method, quick experiments\n",
        "\n",
        "### 2. **Laplacian Noise Injection**\n",
        "   - **Approach**: Add Laplace(0, b) noise to parameters\n",
        "   - **Pros**: Heavier tails â†’ better for differential privacy guarantees\n",
        "   - **Cons**: More variable results, still uniform across parameters\n",
        "   - **Use case**: When formal privacy guarantees are needed\n",
        "\n",
        "### 3. **Adaptive Noise Scaling**\n",
        "   - **Approach**: Scale noise based on gradient magnitude on forget set\n",
        "   - **Pros**: Targets important parameters, better trade-off between forgetting/retaining\n",
        "   - **Cons**: Requires forget set access, more computation\n",
        "   - **Use case**: When you want efficient, targeted unlearning\n",
        "\n",
        "### 4. **Layer-wise Noise Injection**\n",
        "   - **Approach**: Apply different noise levels to different network layers\n",
        "   - **Pros**: Fine-grained control, can preserve low-level features\n",
        "   - **Cons**: Requires understanding of layer hierarchy\n",
        "   - **Use case**: When you know which layers are most responsible for memorization\n",
        "\n",
        "### 5. **Gradient-based Noise**\n",
        "   - **Approach**: Noise magnitude directly proportional to gradient magnitude\n",
        "   - **Pros**: Extremely targeted, follows loss landscape\n",
        "   - **Cons**: Very sensitive to multiplier choice\n",
        "   - **Use case**: Maximum targeting efficiency, research comparisons\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“Š Evaluation Metrics:\n",
        "\n",
        "We measure unlearning success using three key metrics:\n",
        "\n",
        "1. **Forget Set Accuracy** (â†“ lower is better)\n",
        "   - How well the model still predicts the forget set\n",
        "   - Should decrease after successful unlearning\n",
        "   - Target: Approach performance on unseen data\n",
        "\n",
        "2. **Retain Set Accuracy** (â†‘ higher is better)\n",
        "   - How well the model still predicts the retained training data\n",
        "   - Should remain high (minimal collateral damage)\n",
        "   - Target: Stay close to original model performance\n",
        "\n",
        "3. **Test Set Accuracy** (â†‘ higher is better)\n",
        "   - Overall model generalization performance\n",
        "   - Indicates if the model is still useful after unlearning\n",
        "   - Target: Maintain competitive accuracy\n",
        "\n",
        "4. **Parameter Distance** (L2 norm)\n",
        "   - How much parameters changed from original model\n",
        "   - Helps understand aggressiveness of unlearning\n",
        "   - Larger distance = more aggressive modification\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ“ Learning Objectives:\n",
        "\n",
        "By the end of this notebook, you will understand:\n",
        "- âœ… Why machine unlearning is important\n",
        "- âœ… How noise-based methods induce forgetting\n",
        "- âœ… Trade-offs between different noise distributions\n",
        "- âœ… How to evaluate unlearning effectiveness\n",
        "- âœ… When to use each method in practice\n",
        "- âœ… Comparison with \"gold standard\" (retrain from scratch)\n",
        "\n",
        "---\n",
        "\n",
        "Let's dive in! ðŸš€\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "PyTorch version: 2.7.1+cpu\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# LIBRARY IMPORTS AND SETUP\n",
        "# ============================================================================\n",
        "\n",
        "# Import required libraries for data processing and visualization\n",
        "import numpy as np          # Numerical operations and array handling\n",
        "import pandas as pd         # Data manipulation and analysis (for results tables)\n",
        "import matplotlib.pyplot as plt  # Plotting and visualization\n",
        "import seaborn as sns       # Statistical visualization (enhances matplotlib)\n",
        "\n",
        "# PyTorch imports for deep learning\n",
        "import torch                # Core PyTorch library for tensor operations\n",
        "import torch.nn as nn       # Neural network modules (layers, loss functions)\n",
        "import torch.optim as optim # Optimization algorithms (Adam, SGD, etc.)\n",
        "from torch.utils.data import TensorDataset, DataLoader, Subset  # Data handling utilities\n",
        "\n",
        "# Scikit-learn imports for data generation and preprocessing\n",
        "from sklearn.datasets import make_classification  # Generate synthetic classification datasets\n",
        "from sklearn.model_selection import train_test_split  # Split data into train/test sets\n",
        "from sklearn.preprocessing import StandardScaler  # Feature standardization (mean=0, std=1)\n",
        "from sklearn.metrics import accuracy_score, classification_report  # Model evaluation metrics\n",
        "\n",
        "# Utility imports\n",
        "from copy import deepcopy   # Create independent copies of models (important for comparing unlearning methods)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output\n",
        "\n",
        "# ============================================================================\n",
        "# REPRODUCIBILITY: Set random seeds\n",
        "# ============================================================================\n",
        "# Setting seeds ensures that random operations (data generation, weight initialization, etc.)\n",
        "# produce the same results across different runs, making experiments reproducible\n",
        "np.random.seed(42)      # NumPy random operations\n",
        "torch.manual_seed(42)   # PyTorch CPU operations\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)  # PyTorch GPU operations\n",
        "\n",
        "# ============================================================================\n",
        "# DEVICE CONFIGURATION\n",
        "# ============================================================================\n",
        "# Use GPU if available for faster computation, otherwise use CPU\n",
        "# For this tabular data task, CPU is usually sufficient\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Generate Tabular Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (2000, 20)\n",
            "Number of features: 20\n",
            "Class distribution: [ 986 1014]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# DATASET GENERATION\n",
        "# ============================================================================\n",
        "# Create a synthetic binary classification dataset to simulate real-world tabular data\n",
        "# We use synthetic data because it allows us to control all aspects of the dataset\n",
        "\n",
        "# Dataset parameters\n",
        "n_samples = 2000         # Total number of samples (moderate size for quick experimentation)\n",
        "n_features = 20          # Number of input features (typical for tabular data)\n",
        "n_informative = 15       # Features that actually contribute to the classification (75%)\n",
        "n_redundant = 3          # Features that are linear combinations of informative features\n",
        "                         # Remaining features (20-15-3=2) will be random noise\n",
        "\n",
        "# Generate synthetic dataset using sklearn's make_classification\n",
        "# This creates a realistic classification problem with controlled characteristics\n",
        "X, y = make_classification(\n",
        "    n_samples=n_samples,\n",
        "    n_features=n_features,\n",
        "    n_informative=n_informative,  # These features have predictive power\n",
        "    n_redundant=n_redundant,      # These add realistic correlation between features\n",
        "    n_classes=2,                  # Binary classification (0 or 1)\n",
        "    random_state=42,              # For reproducibility\n",
        "    flip_y=0.1                    # Flip 10% of labels to add noise (makes problem more realistic)\n",
        ")\n",
        "\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Number of features: {n_features}\")\n",
        "print(f\"Class distribution: {np.bincount(y)}\")  # Check if classes are balanced\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Data splits:\n",
            "Retain set: 1440 samples\n",
            "Forget set: 160 samples\n",
            "Test set: 400 samples\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# DATA SPLITTING STRATEGY\n",
        "# ============================================================================\n",
        "# Machine unlearning requires three distinct sets:\n",
        "# 1. RETAIN SET: Data the model should continue to remember\n",
        "# 2. FORGET SET: Data the model should \"forget\" (simulate data deletion request)\n",
        "# 3. TEST SET: Unseen data to evaluate overall model performance\n",
        "\n",
        "# First split: Separate test set (20% of all data)\n",
        "# We use stratify=y to maintain class balance in all splits\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2,  # 20% for testing (400 samples)\n",
        "    random_state=42, \n",
        "    stratify=y  # Ensures both train and test have similar class distributions\n",
        ")\n",
        "\n",
        "# Second split: Divide training set into FORGET and RETAIN sets\n",
        "# The forget set represents data that a user wants removed (e.g., GDPR right to be forgotten)\n",
        "forget_ratio = 0.1  # 10% of training data will be the \"forget set\" (typical in unlearning research)\n",
        "                    # This means 90% remains as the \"retain set\"\n",
        "X_retain, X_forget, y_retain, y_forget = train_test_split(\n",
        "    X_train, y_train, \n",
        "    test_size=forget_ratio,  # 10% for forgetting (160 samples)\n",
        "    random_state=42, \n",
        "    stratify=y_train  # Maintain class balance in both sets\n",
        ")\n",
        "\n",
        "print(f\"\\nData splits:\")\n",
        "print(f\"Retain set: {X_retain.shape[0]} samples\")  # 1440 samples (72% of total)\n",
        "print(f\"Forget set: {X_forget.shape[0]} samples\")  # 160 samples (8% of total)\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")      # 400 samples (20% of total)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# FEATURE STANDARDIZATION\n",
        "# ============================================================================\n",
        "# Neural networks perform better when input features have similar scales (mean=0, std=1)\n",
        "# This prevents features with larger magnitudes from dominating the learning process\n",
        "\n",
        "scaler = StandardScaler()  # Computes mean and std for scaling\n",
        "\n",
        "# IMPORTANT: Fit scaler ONLY on retain set\n",
        "# Why? In real unlearning scenarios, we pretend the forget set never existed\n",
        "# Fitting on retain set only makes the experiment more realistic\n",
        "X_retain_scaled = scaler.fit_transform(X_retain)  # Fit and transform retain set\n",
        "\n",
        "# Transform (but don't fit) forget and test sets using retain set's statistics\n",
        "# This ensures we use the same scaling parameters learned from retain set\n",
        "X_forget_scaled = scaler.transform(X_forget)  # Apply same scaling\n",
        "X_test_scaled = scaler.transform(X_test)      # Apply same scaling\n",
        "\n",
        "# ============================================================================\n",
        "# CONVERT TO PYTORCH TENSORS\n",
        "# ============================================================================\n",
        "# PyTorch requires data in tensor format (similar to NumPy arrays but with GPU support)\n",
        "\n",
        "# FloatTensor for input features (continuous values)\n",
        "X_retain_tensor = torch.FloatTensor(X_retain_scaled).to(device)\n",
        "X_forget_tensor = torch.FloatTensor(X_forget_scaled).to(device)\n",
        "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
        "\n",
        "# LongTensor for labels (class indices: 0 or 1)\n",
        "# CrossEntropyLoss in PyTorch expects labels as LongTensor\n",
        "y_retain_tensor = torch.LongTensor(y_retain).to(device)\n",
        "y_forget_tensor = torch.LongTensor(y_forget).to(device)\n",
        "y_test_tensor = torch.LongTensor(y_test).to(device)\n",
        "\n",
        "print(\"\\nData preparation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Define Neural Network Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# NEURAL NETWORK MODEL DEFINITION\n",
        "# ============================================================================\n",
        "\n",
        "class TabularClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Deep neural network for binary classification on tabular data.\n",
        "    \n",
        "    Architecture Design Decisions:\n",
        "    - Multiple hidden layers with decreasing sizes (64 -> 32 -> 16) create a \"funnel\" architecture\n",
        "    - This progressively compresses information and learns hierarchical features\n",
        "    - BatchNorm helps with training stability and faster convergence\n",
        "    - ReLU activation introduces non-linearity (allows learning complex patterns)\n",
        "    - Dropout prevents overfitting by randomly deactivating neurons during training\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden_sizes=[64, 32, 16], num_classes=2, dropout=0.3):\n",
        "        \"\"\"\n",
        "        Initialize the neural network architecture.\n",
        "        \n",
        "        Args:\n",
        "            input_size: Number of input features (20 in our case)\n",
        "            hidden_sizes: List of hidden layer sizes (progressive decrease is common)\n",
        "            num_classes: Number of output classes (2 for binary classification)\n",
        "            dropout: Dropout probability (0.3 = 30% of neurons randomly deactivated)\n",
        "                     - Too low (< 0.2): May overfit\n",
        "                     - Too high (> 0.5): May underfit\n",
        "                     - 0.3 is a good default for moderate regularization\n",
        "        \"\"\"\n",
        "        super(TabularClassifier, self).__init__()\n",
        "        \n",
        "        layers = []  # Store all layers in a list\n",
        "        prev_size = input_size  # Track input size for next layer\n",
        "        \n",
        "        # ========================================================================\n",
        "        # BUILD HIDDEN LAYERS\n",
        "        # ========================================================================\n",
        "        # Each hidden layer block consists of 4 components:\n",
        "        for hidden_size in hidden_sizes:\n",
        "            # 1. Linear (fully connected) layer: y = Wx + b\n",
        "            #    This is where the actual learning happens through weights W and bias b\n",
        "            layers.append(nn.Linear(prev_size, hidden_size))\n",
        "            \n",
        "            # 2. Batch Normalization: Normalizes activations (mean=0, std=1)\n",
        "            #    Benefits:\n",
        "            #    - Reduces internal covariate shift (stabilizes learning)\n",
        "            #    - Allows higher learning rates\n",
        "            #    - Acts as mild regularization\n",
        "            layers.append(nn.BatchNorm1d(hidden_size))\n",
        "            \n",
        "            # 3. ReLU Activation: f(x) = max(0, x)\n",
        "            #    Why ReLU?\n",
        "            #    - Computationally efficient (just thresholding at 0)\n",
        "            #    - Helps avoid vanishing gradient problem\n",
        "            #    - Introduces non-linearity (without it, network would just be linear regression)\n",
        "            layers.append(nn.ReLU())\n",
        "            \n",
        "            # 4. Dropout: Randomly set activations to 0 with probability p\n",
        "            #    Why use it?\n",
        "            #    - Prevents co-adaptation of neurons (forces redundancy)\n",
        "            #    - Acts as ensemble of sub-networks\n",
        "            #    - Only active during training (automatically disabled during evaluation)\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "            \n",
        "            prev_size = hidden_size  # Update for next layer\n",
        "        \n",
        "        # ========================================================================\n",
        "        # OUTPUT LAYER\n",
        "        # ========================================================================\n",
        "        # Final linear layer: maps to number of classes (2 for binary)\n",
        "        # No activation here because CrossEntropyLoss includes softmax internally\n",
        "        layers.append(nn.Linear(prev_size, num_classes))\n",
        "        \n",
        "        # Combine all layers into a sequential container\n",
        "        # This allows easy forward pass: just call self.network(x)\n",
        "        self.network = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass: compute predictions from input.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, input_size)\n",
        "        \n",
        "        Returns:\n",
        "            Output tensor of shape (batch_size, num_classes)\n",
        "            Contains raw logits (not probabilities)\n",
        "        \"\"\"\n",
        "        return self.network(x)\n",
        "    \n",
        "    def get_layer_names(self):\n",
        "        \"\"\"\n",
        "        Get names of linear layers for targeted noise injection.\n",
        "        Used by layer-wise unlearning methods to selectively add noise to specific layers.\n",
        "        \"\"\"\n",
        "        return [name for name, module in self.named_modules() if isinstance(module, nn.Linear)]\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL INSTANTIATION\n",
        "# ============================================================================\n",
        "# Create the model with specified architecture\n",
        "model = TabularClassifier(\n",
        "    input_size=n_features,     # 20 input features\n",
        "    hidden_sizes=[64, 32, 16], # Three hidden layers with decreasing sizes\n",
        "                                # Why this architecture?\n",
        "                                # - 64: First layer extracts basic patterns\n",
        "                                # - 32: Second layer combines basic patterns\n",
        "                                # - 16: Third layer creates high-level representations\n",
        "    num_classes=2,             # Binary classification (0 or 1)\n",
        "    dropout=0.3                # 30% dropout for regularization\n",
        ").to(device)  # Move model to GPU if available\n",
        "\n",
        "print(\"Model Architecture:\")\n",
        "print(model)\n",
        "\n",
        "# Calculate total number of learnable parameters\n",
        "# More parameters = more capacity but also more risk of overfitting\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nTotal parameters: {total_params}\")\n",
        "print(f\"Trainable parameters: {trainable_params}\")\n",
        "\n",
        "# Typical parameter count for this architecture:\n",
        "# Layer 1: (20 * 64) + 64 = 1,344 parameters (weights + bias)\n",
        "# Layer 2: (64 * 32) + 32 = 2,080 parameters\n",
        "# Layer 3: (32 * 16) + 16 = 528 parameters\n",
        "# Layer 4: (16 * 2) + 2 = 34 parameters\n",
        "# Plus BatchNorm parameters (2 per layer: gamma and beta)\n",
        "# Total: ~4,000-5,000 parameters (moderate size, good for our dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Training and Evaluation Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TRAINING AND EVALUATION UTILITY FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def train_model(model, X_train, y_train, epochs=50, batch_size=32, lr=0.001, verbose=True):\n",
        "    \"\"\"\n",
        "    Train the neural network using standard supervised learning.\n",
        "    \n",
        "    Args:\n",
        "        model: The neural network to train\n",
        "        X_train: Training features (tensor)\n",
        "        y_train: Training labels (tensor)\n",
        "        epochs: Number of complete passes through the dataset\n",
        "                - More epochs = more learning but risk of overfitting\n",
        "                - We use 50-100 epochs as a good balance\n",
        "        batch_size: Number of samples processed before updating weights\n",
        "                    - Smaller batches (16-32): More noise, better generalization, slower\n",
        "                    - Larger batches (128-256): More stable, faster, may overfit\n",
        "                    - 32 is a good default for small-medium datasets\n",
        "        lr: Learning rate for Adam optimizer\n",
        "            - Too high (>0.01): Training may diverge (loss explodes)\n",
        "            - Too low (<0.0001): Training too slow, may get stuck\n",
        "            - 0.001 is a safe default for Adam\n",
        "        verbose: Whether to print training progress\n",
        "    \n",
        "    Returns:\n",
        "        losses: List of average loss per epoch (for plotting convergence)\n",
        "    \"\"\"\n",
        "    model.train()  # Set model to training mode (enables dropout and batch norm updates)\n",
        "    \n",
        "    # ========================================================================\n",
        "    # LOSS FUNCTION\n",
        "    # ========================================================================\n",
        "    # CrossEntropyLoss combines log_softmax and negative log likelihood\n",
        "    # It's the standard loss for multi-class classification\n",
        "    # For binary classification, it automatically handles both classes\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # ========================================================================\n",
        "    # OPTIMIZER: Adam (Adaptive Moment Estimation)\n",
        "    # ========================================================================\n",
        "    # Why Adam over SGD?\n",
        "    # - Adapts learning rate per parameter (good for sparse gradients)\n",
        "    # - Includes momentum (accelerates convergence)\n",
        "    # - Less sensitive to learning rate choice\n",
        "    # - Generally works well out-of-the-box\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    # ========================================================================\n",
        "    # DATA LOADER\n",
        "    # ========================================================================\n",
        "    # DataLoader handles batching and shuffling automatically\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, \n",
        "        batch_size=batch_size,  # Process 32 samples at a time\n",
        "        shuffle=True  # Shuffle data each epoch (prevents learning order-dependent patterns)\n",
        "    )\n",
        "    \n",
        "    losses = []  # Track loss over time to monitor convergence\n",
        "    \n",
        "    # ========================================================================\n",
        "    # TRAINING LOOP\n",
        "    # ========================================================================\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0.0\n",
        "        \n",
        "        # Process data in mini-batches\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            # Step 1: Clear gradients from previous batch\n",
        "            # (PyTorch accumulates gradients by default)\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Step 2: Forward pass - compute predictions\n",
        "            outputs = model(batch_X)\n",
        "            \n",
        "            # Step 3: Compute loss - how wrong are the predictions?\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            \n",
        "            # Step 4: Backward pass - compute gradients via backpropagation\n",
        "            # This calculates âˆ‚loss/âˆ‚weight for all parameters\n",
        "            loss.backward()\n",
        "            \n",
        "            # Step 5: Update parameters using gradients\n",
        "            # New weight = Old weight - learning_rate * gradient\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss.item()  # Accumulate loss for this epoch\n",
        "        \n",
        "        # Calculate average loss across all batches\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        losses.append(avg_loss)\n",
        "        \n",
        "        # Print progress every 10 epochs\n",
        "        if verbose and (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}')\n",
        "    \n",
        "    return losses\n",
        "\n",
        "def evaluate_model(model, X, y):\n",
        "    \"\"\"\n",
        "    Evaluate model accuracy on a dataset.\n",
        "    \n",
        "    This is used to measure:\n",
        "    - Retain set accuracy: How well model remembers retained data\n",
        "    - Forget set accuracy: How well model remembers forgotten data (should decrease after unlearning)\n",
        "    - Test set accuracy: Overall model performance on unseen data\n",
        "    \n",
        "    Args:\n",
        "        model: Trained neural network\n",
        "        X: Features to evaluate on\n",
        "        y: True labels\n",
        "    \n",
        "    Returns:\n",
        "        accuracy: Percentage of correctly classified samples (0-100)\n",
        "    \"\"\"\n",
        "    model.eval()  # Set to evaluation mode (disables dropout, uses batch norm running stats)\n",
        "    \n",
        "    # torch.no_grad() disables gradient computation (saves memory and speeds up inference)\n",
        "    with torch.no_grad():\n",
        "        # Get model predictions\n",
        "        outputs = model(X)  # Shape: (n_samples, n_classes)\n",
        "        \n",
        "        # Get predicted class (argmax of output logits)\n",
        "        # outputs.data contains raw logits for each class\n",
        "        # torch.max returns (max_values, indices) - we only need indices\n",
        "        _, predicted = torch.max(outputs.data, 1)  # 1 means take max along class dimension\n",
        "        \n",
        "        # Calculate accuracy\n",
        "        correct = (predicted == y).sum().item()  # Count correct predictions\n",
        "        total = y.size(0)  # Total number of samples\n",
        "        accuracy = 100 * correct / total\n",
        "    \n",
        "    return accuracy\n",
        "\n",
        "def get_model_state_dict_copy(model):\n",
        "    \"\"\"\n",
        "    Create a deep copy of model's parameters (weights and biases).\n",
        "    \n",
        "    Why do we need this?\n",
        "    - To save original model state before applying unlearning\n",
        "    - To compare how much parameters changed after unlearning\n",
        "    - .clone().detach() creates independent copy (changes won't affect original)\n",
        "    \n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary mapping parameter names to their cloned tensors\n",
        "    \"\"\"\n",
        "    return {name: param.clone().detach() for name, param in model.state_dict().items()}\n",
        "\n",
        "def calculate_parameter_distance(state_dict1, state_dict2):\n",
        "    \"\"\"\n",
        "    Calculate L2 (Euclidean) distance between two sets of model parameters.\n",
        "    \n",
        "    This metric tells us:\n",
        "    - How much the model changed after unlearning\n",
        "    - Larger distance = more aggressive unlearning\n",
        "    - Can help find optimal noise level (balance between forgetting and retaining)\n",
        "    \n",
        "    The L2 distance is computed as:\n",
        "    distance = sqrt(sum((param1 - param2)^2))\n",
        "    \n",
        "    We only consider weights and biases (not batch norm statistics).\n",
        "    \n",
        "    Args:\n",
        "        state_dict1: First model's parameters (e.g., original model)\n",
        "        state_dict2: Second model's parameters (e.g., after unlearning)\n",
        "    \n",
        "    Returns:\n",
        "        distance: Scalar value representing total parameter change\n",
        "    \"\"\"\n",
        "    distance = 0.0\n",
        "    \n",
        "    # Iterate through all parameters\n",
        "    for key in state_dict1.keys():\n",
        "        # Only compare weights and biases (skip batch norm running stats)\n",
        "        if 'weight' in key or 'bias' in key:\n",
        "            # Compute squared difference for this parameter\n",
        "            param_diff = state_dict1[key] - state_dict2[key]\n",
        "            # Add squared norm to total distance\n",
        "            distance += torch.norm(param_diff).item() ** 2\n",
        "    \n",
        "    # Return square root of sum of squared differences (L2 norm)\n",
        "    return np.sqrt(distance)\n",
        "\n",
        "print(\"Training and evaluation functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train Original Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TRAIN ORIGINAL MODEL (BASELINE)\n",
        "# ============================================================================\n",
        "# This model will serve as our baseline for comparison\n",
        "# It's trained on ALL data (retain + forget sets)\n",
        "# Later, we'll try to make it \"forget\" the forget set using various noise methods\n",
        "\n",
        "# Combine retain and forget sets to create full training set\n",
        "# Why? In real scenarios, the model was originally trained on all data\n",
        "# The forget set represents data we want to remove AFTER the fact\n",
        "X_full_train = torch.cat([X_retain_tensor, X_forget_tensor], dim=0)  # Concatenate features\n",
        "y_full_train = torch.cat([y_retain_tensor, y_forget_tensor], dim=0)  # Concatenate labels\n",
        "# Result: 1600 samples total (1440 retain + 160 forget)\n",
        "\n",
        "print(\"Training original model on full training set...\")\n",
        "# Train for 100 epochs with learning rate 0.001\n",
        "# More epochs (100 vs 50) ensures model fully learns the data\n",
        "# This makes unlearning more challenging (stronger memorization to overcome)\n",
        "train_losses = train_model(model, X_full_train, y_full_train, epochs=100, lr=0.001)\n",
        "\n",
        "# ========================================================================\n",
        "# EVALUATE ORIGINAL MODEL PERFORMANCE\n",
        "# ========================================================================\n",
        "# We evaluate on all three sets to establish baseline metrics\n",
        "# These will be compared against unlearned models\n",
        "\n",
        "# RETAIN SET: Should be high (model learned this data)\n",
        "retain_acc = evaluate_model(model, X_retain_tensor, y_retain_tensor)\n",
        "\n",
        "# FORGET SET: Will also be high initially (model learned this too)\n",
        "# After unlearning, we expect this to DECREASE\n",
        "forget_acc = evaluate_model(model, X_forget_tensor, y_forget_tensor)\n",
        "\n",
        "# TEST SET: Overall generalization performance\n",
        "test_acc = evaluate_model(model, X_test_tensor, y_test_tensor)\n",
        "\n",
        "print(\"\\n=== Original Model Performance ===\")\n",
        "print(f\"Retain Set Accuracy: {retain_acc:.2f}%\")\n",
        "print(f\"Forget Set Accuracy: {forget_acc:.2f}%\")  # This is our target to reduce\n",
        "print(f\"Test Set Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "# ========================================================================\n",
        "# SAVE ORIGINAL MODEL STATE\n",
        "# ========================================================================\n",
        "# Critical: Save a copy of parameters BEFORE any unlearning\n",
        "# This allows us to:\n",
        "# 1. Compare how much parameters changed (parameter distance metric)\n",
        "# 2. Restore original model if needed\n",
        "# 3. Repeatedly apply different unlearning methods to same baseline\n",
        "original_model_state = get_model_state_dict_copy(model)\n",
        "\n",
        "# ========================================================================\n",
        "# VISUALIZE TRAINING CONVERGENCE\n",
        "# ========================================================================\n",
        "# Plot loss curve to verify training converged properly\n",
        "# Should show decreasing trend (model is learning)\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(train_losses, linewidth=2, color='#1f77b4')\n",
        "plt.title('Original Model Training Loss', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Loss', fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# EXPECTED PATTERN:\n",
        "# - Loss should decrease rapidly in first 20-30 epochs\n",
        "# - Then plateau as model converges\n",
        "# - If loss is still decreasing at epoch 100, could train longer\n",
        "# - If loss is flat from the start, learning rate might be too low\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Machine Unlearning Methods\n",
        "\n",
        "### Method 1: Gaussian Noise Injection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# METHOD 1: GAUSSIAN NOISE INJECTION\n",
        "# ============================================================================\n",
        "\n",
        "def gaussian_noise_unlearning(model, sigma=0.01):\n",
        "    \"\"\"\n",
        "    Add Gaussian (normal) noise to all model parameters to induce forgetting.\n",
        "    \n",
        "    INTUITION: Why does adding noise cause forgetting?\n",
        "    - Neural networks store information in their parameters (weights and biases)\n",
        "    - Adding random noise \"corrupts\" this stored information\n",
        "    - Small perturbations disrupt the precise parameter values needed for memorization\n",
        "    - The model becomes less confident about specific data points it memorized\n",
        "    \n",
        "    GAUSSIAN DISTRIBUTION: N(0, ÏƒÂ²)\n",
        "    - Mean = 0: Noise is centered (no systematic bias)\n",
        "    - Variance = ÏƒÂ²: Controls noise magnitude\n",
        "    - 68% of values within Â±Ïƒ, 95% within Â±2Ïƒ (bell curve)\n",
        "    \n",
        "    WHY GAUSSIAN?\n",
        "    - Symmetric: Equally likely to increase or decrease parameters\n",
        "    - Well-studied: Clear statistical properties\n",
        "    - Central Limit Theorem: Sum of many small effects is Gaussian\n",
        "    - Simple to implement and tune (just one parameter: Ïƒ)\n",
        "    \n",
        "    TRADE-OFF:\n",
        "    - Larger Ïƒ â†’ More forgetting but also damages retain set performance\n",
        "    - Smaller Ïƒ â†’ Preserves retain set but less effective forgetting\n",
        "    \n",
        "    Args:\n",
        "        model: PyTorch model to apply unlearning to\n",
        "        sigma: Standard deviation of Gaussian noise\n",
        "               - 0.001: Very conservative, minimal forgetting\n",
        "               - 0.01: Moderate, good balance\n",
        "               - 0.1: Aggressive, may hurt retain set significantly\n",
        "    \n",
        "    Returns:\n",
        "        unlearned_model: New model with noise-perturbed parameters\n",
        "    \"\"\"\n",
        "    # Create independent copy of model (original stays unchanged)\n",
        "    unlearned_model = deepcopy(model)\n",
        "    \n",
        "    # Disable gradient tracking (we're not training, just modifying parameters)\n",
        "    with torch.no_grad():\n",
        "        for name, param in unlearned_model.named_parameters():\n",
        "            # Only add noise to weights and biases (the learnable parameters)\n",
        "            # Skip batch norm running statistics (not learnable, just tracking stats)\n",
        "            if 'weight' in name or 'bias' in name:\n",
        "                # Generate Gaussian noise with same shape as parameter\n",
        "                # torch.randn_like: samples from N(0, 1), then scale by sigma\n",
        "                noise = torch.randn_like(param) * sigma\n",
        "                \n",
        "                # Add noise to parameter in-place\n",
        "                # param.add_() is more efficient than param = param + noise\n",
        "                param.add_(noise)\n",
        "    \n",
        "    return unlearned_model\n",
        "\n",
        "# ============================================================================\n",
        "# EXPERIMENT: Test different noise levels\n",
        "# ============================================================================\n",
        "# We test multiple sigma values to find the optimal trade-off between:\n",
        "# 1. Forgetting effectiveness (lower forget set accuracy)\n",
        "# 2. Retain set preservation (maintain high retain set accuracy)\n",
        "\n",
        "sigma_values = [0.001, 0.005, 0.01, 0.05, 0.1]  # Range from very conservative to aggressive\n",
        "gaussian_results = []  # Store results for comparison\n",
        "\n",
        "print(\"Testing Gaussian Noise Unlearning...\\n\")\n",
        "\n",
        "for sigma in sigma_values:\n",
        "    # Apply unlearning with current sigma\n",
        "    unlearned_model = gaussian_noise_unlearning(model, sigma=sigma)\n",
        "    \n",
        "    # Evaluate on all three sets\n",
        "    # RETAIN SET: Should stay high (we want to keep this knowledge)\n",
        "    retain_acc = evaluate_model(unlearned_model, X_retain_tensor, y_retain_tensor)\n",
        "    \n",
        "    # FORGET SET: Should decrease (successful unlearning)\n",
        "    forget_acc = evaluate_model(unlearned_model, X_forget_tensor, y_forget_tensor)\n",
        "    \n",
        "    # TEST SET: Overall performance indicator\n",
        "    test_acc = evaluate_model(unlearned_model, X_test_tensor, y_test_tensor)\n",
        "    \n",
        "    # Calculate how much parameters changed from original\n",
        "    # Larger distance = more aggressive modification\n",
        "    param_dist = calculate_parameter_distance(\n",
        "        original_model_state,\n",
        "        get_model_state_dict_copy(unlearned_model)\n",
        "    )\n",
        "    \n",
        "    # Store results\n",
        "    gaussian_results.append({\n",
        "        'sigma': sigma,\n",
        "        'retain_acc': retain_acc,\n",
        "        'forget_acc': forget_acc,\n",
        "        'test_acc': test_acc,\n",
        "        'param_dist': param_dist\n",
        "    })\n",
        "    \n",
        "    # Print summary for this sigma value\n",
        "    print(f\"Sigma={sigma:.3f}: Retain={retain_acc:.2f}%, Forget={forget_acc:.2f}%, Test={test_acc:.2f}%, Dist={param_dist:.4f}\")\n",
        "\n",
        "# Convert to DataFrame for easier analysis and visualization\n",
        "gaussian_df = pd.DataFrame(gaussian_results)\n",
        "print(\"\\nGaussian Noise Unlearning Results:\")\n",
        "print(gaussian_df)\n",
        "\n",
        "# EXPECTED PATTERN:\n",
        "# - As sigma increases, forget_acc should decrease (more forgetting)\n",
        "# - As sigma increases, retain_acc should also decrease (collateral damage)\n",
        "# - Optimal sigma balances these two effects\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Method 2: Laplacian Noise Injection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# METHOD 2: LAPLACIAN NOISE INJECTION\n",
        "# ============================================================================\n",
        "\n",
        "def laplacian_noise_unlearning(model, scale=0.01):\n",
        "    \"\"\"\n",
        "    Add Laplacian (double exponential) noise to model parameters.\n",
        "    \n",
        "    WHY LAPLACIAN INSTEAD OF GAUSSIAN?\n",
        "    \n",
        "    1. DIFFERENTIAL PRIVACY:\n",
        "       - Laplacian noise provides formal differential privacy guarantees\n",
        "       - Used in standard DP-SGD (Differentially Private Stochastic Gradient Descent)\n",
        "       - Protects individual data points from being inferred from model parameters\n",
        "    \n",
        "    2. HEAVIER TAILS:\n",
        "       - Laplacian distribution has heavier tails than Gaussian\n",
        "       - More probability mass at extreme values\n",
        "       - This means: occasionally adds large perturbations (stronger forgetting)\n",
        "       - But median is closer to 0 (less damage to important parameters)\n",
        "    \n",
        "    3. SHAPE COMPARISON:\n",
        "       - Gaussian: exp(-xÂ²/2ÏƒÂ²) â†’ bell curve, rapid decay\n",
        "       - Laplacian: exp(-|x|/b) â†’ peaked center, slower decay (heavier tails)\n",
        "    \n",
        "    MATHEMATICAL PROPERTIES:\n",
        "    - Mean: 0 (centered, unbiased)\n",
        "    - Variance: 2bÂ² (where b is the scale parameter)\n",
        "    - Peak at 0 is higher than Gaussian (more values near original)\n",
        "    - Tails decay linearly in log space (vs quadratic for Gaussian)\n",
        "    \n",
        "    WHEN TO USE LAPLACIAN:\n",
        "    - When you need formal privacy guarantees (GDPR, HIPAA compliance)\n",
        "    - When you want more targeted forgetting (preserve important parameters, disrupt less important ones)\n",
        "    - In research papers for direct comparison with DP methods\n",
        "    \n",
        "    Args:\n",
        "        model: PyTorch model to apply unlearning to\n",
        "        scale: Scale parameter 'b' of Laplacian distribution\n",
        "               - Variance = 2 * scaleÂ²\n",
        "               - To match Gaussian Ïƒ, set scale â‰ˆ Ïƒ/âˆš2 â‰ˆ 0.7Ïƒ\n",
        "    \n",
        "    Returns:\n",
        "        unlearned_model: New model with Laplacian noise-perturbed parameters\n",
        "    \"\"\"\n",
        "    # Create independent copy of model\n",
        "    unlearned_model = deepcopy(model)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for name, param in unlearned_model.named_parameters():\n",
        "            if 'weight' in name or 'bias' in name:\n",
        "                # Generate Laplacian noise using NumPy (PyTorch doesn't have built-in Laplacian)\n",
        "                # np.random.laplace(loc=0, scale=scale, size=shape)\n",
        "                # loc=0: center at zero (unbiased)\n",
        "                # scale: controls spread (analogous to Ïƒ in Gaussian)\n",
        "                noise = torch.from_numpy(\n",
        "                    np.random.laplace(0, scale, param.shape)  # Generate as NumPy array\n",
        "                ).float().to(param.device)  # Convert to PyTorch tensor, move to correct device\n",
        "                \n",
        "                # Add noise to parameter\n",
        "                param.add_(noise)\n",
        "    \n",
        "    return unlearned_model\n",
        "\n",
        "# ============================================================================\n",
        "# EXPERIMENT: Test different scale values\n",
        "# ============================================================================\n",
        "# Compare Laplacian to Gaussian at similar noise levels\n",
        "# Note: Laplacian with scale=b has variance 2bÂ², while Gaussian with Ïƒ has variance ÏƒÂ²\n",
        "# So Laplacian(scale=0.01) has more variance than Gaussian(Ïƒ=0.01)\n",
        "\n",
        "scale_values = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
        "laplacian_results = []\n",
        "\n",
        "print(\"Testing Laplacian Noise Unlearning...\\n\")\n",
        "\n",
        "for scale in scale_values:\n",
        "    # Apply unlearning\n",
        "    unlearned_model = laplacian_noise_unlearning(model, scale=scale)\n",
        "    \n",
        "    # Evaluate on all three sets\n",
        "    retain_acc = evaluate_model(unlearned_model, X_retain_tensor, y_retain_tensor)\n",
        "    forget_acc = evaluate_model(unlearned_model, X_forget_tensor, y_forget_tensor)\n",
        "    test_acc = evaluate_model(unlearned_model, X_test_tensor, y_test_tensor)\n",
        "    \n",
        "    # Calculate parameter distance\n",
        "    param_dist = calculate_parameter_distance(\n",
        "        original_model_state,\n",
        "        get_model_state_dict_copy(unlearned_model)\n",
        "    )\n",
        "    \n",
        "    laplacian_results.append({\n",
        "        'scale': scale,\n",
        "        'retain_acc': retain_acc,\n",
        "        'forget_acc': forget_acc,\n",
        "        'test_acc': test_acc,\n",
        "        'param_dist': param_dist\n",
        "    })\n",
        "    \n",
        "    print(f\"Scale={scale:.3f}: Retain={retain_acc:.2f}%, Forget={forget_acc:.2f}%, Test={test_acc:.2f}%, Dist={param_dist:.4f}\")\n",
        "\n",
        "laplacian_df = pd.DataFrame(laplacian_results)\n",
        "print(\"\\nLaplacian Noise Unlearning Results:\")\n",
        "print(laplacian_df)\n",
        "\n",
        "# COMPARISON WITH GAUSSIAN:\n",
        "# - Laplacian may show more variable results due to heavier tails\n",
        "# - For same scale/sigma, Laplacian typically causes more parameter change\n",
        "# - Laplacian may better preserve retain accuracy (peaked center)\n",
        "# - But also may have more aggressive forgetting (heavy tails)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Method 3: Adaptive Noise Scaling (Parameter Importance-Based)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# METHOD 3: ADAPTIVE NOISE SCALING (PARAMETER IMPORTANCE-BASED)\n",
        "# ============================================================================\n",
        "\n",
        "def adaptive_noise_unlearning(model, X_forget, y_forget, base_sigma=0.01, importance_weight=2.0):\n",
        "    \"\"\"\n",
        "    Add noise scaled by parameter importance (gradient magnitude on forget set).\n",
        "    \n",
        "    KEY INSIGHT: Not all parameters are equally responsible for memorizing the forget set!\n",
        "    \n",
        "    MOTIVATION:\n",
        "    - Uniform noise (Gaussian/Laplacian) treats all parameters equally\n",
        "    - But some parameters strongly encode forget set information (high gradients)\n",
        "    - Others barely affect forget set predictions (low gradients)\n",
        "    - Solution: Add MORE noise to important parameters, LESS to others\n",
        "    \n",
        "    HOW IT WORKS:\n",
        "    1. Compute loss on forget set\n",
        "    2. Backpropagate to get gradients: âˆ‚loss/âˆ‚weight\n",
        "    3. Gradient magnitude indicates \"how much this parameter affects forget set\"\n",
        "    4. Scale noise proportionally: high gradient â†’ high noise\n",
        "    \n",
        "    WHY GRADIENTS INDICATE IMPORTANCE?\n",
        "    - Large gradient means: small change to parameter causes large change to loss\n",
        "    - This parameter is \"encoding\" forget set information\n",
        "    - Perturbing it will disrupt forget set memorization\n",
        "    - Small gradient means: parameter doesn't care about forget set\n",
        "    - Less noise needed (preserve retain set performance)\n",
        "    \n",
        "    FORMULA:\n",
        "    adaptive_Ïƒ = base_Ïƒ Ã— (1 + importance_weight Ã— normalized_gradient)\n",
        "    \n",
        "    - normalized_gradient âˆˆ [0, 1]: gradient magnitude relative to max gradient\n",
        "    - importance_weight: amplification factor (how much to increase noise for important params)\n",
        "      * weight=0: uniform noise (same as Gaussian method)\n",
        "      * weight=1: double noise for most important parameters\n",
        "      * weight=2: triple noise for most important parameters\n",
        "    \n",
        "    ADVANTAGES:\n",
        "    - More targeted forgetting (focus on relevant parameters)\n",
        "    - Better preserve retain set (less noise on unimportant parameters)\n",
        "    - Theoretically more efficient than uniform noise\n",
        "    \n",
        "    DISADVANTAGES:\n",
        "    - Requires forward/backward pass on forget set (more computation)\n",
        "    - Needs access to forget set (not always available)\n",
        "    - More hyperparameters to tune\n",
        "    \n",
        "    Args:\n",
        "        model: PyTorch model to apply unlearning to\n",
        "        X_forget: Forget set features (needed to compute gradients)\n",
        "        y_forget: Forget set labels (needed to compute loss)\n",
        "        base_sigma: Minimum noise level (applied to all parameters)\n",
        "        importance_weight: Multiplier for gradient-based scaling\n",
        "                          - Higher values = more aggressive targeting\n",
        "                          - 2.0 is a good default (3x noise for max gradient params)\n",
        "    \n",
        "    Returns:\n",
        "        unlearned_model: Model with adaptive noise-perturbed parameters\n",
        "    \"\"\"\n",
        "    # Create independent copy\n",
        "    unlearned_model = deepcopy(model)\n",
        "    unlearned_model.eval()  # Set to eval mode (use this for gradient computation)\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 1: Compute gradients on forget set\n",
        "    # ========================================================================\n",
        "    # This tells us which parameters are most responsible for forget set predictions\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    unlearned_model.zero_grad()  # Clear any existing gradients\n",
        "    \n",
        "    # Forward pass on forget set\n",
        "    outputs = unlearned_model(X_forget)\n",
        "    \n",
        "    # Compute loss (how well model predicts forget set)\n",
        "    loss = criterion(outputs, y_forget)\n",
        "    \n",
        "    # Backward pass to compute gradients\n",
        "    loss.backward()  # Now param.grad contains âˆ‚loss/âˆ‚param for each parameter\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 2: Extract and normalize gradient magnitudes\n",
        "    # ========================================================================\n",
        "    grad_magnitudes = {}\n",
        "    for name, param in unlearned_model.named_parameters():\n",
        "        if param.grad is not None and ('weight' in name or 'bias' in name):\n",
        "            # Use mean absolute gradient as importance measure\n",
        "            # .abs() because we care about magnitude, not direction\n",
        "            # .mean() to get single value per parameter tensor\n",
        "            grad_magnitudes[name] = param.grad.abs().mean().item()\n",
        "    \n",
        "    # Normalize to [0, 1] range (relative importance)\n",
        "    max_grad = max(grad_magnitudes.values()) if grad_magnitudes else 1.0\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 3: Add adaptive noise based on importance\n",
        "    # ========================================================================\n",
        "    with torch.no_grad():\n",
        "        for name, param in unlearned_model.named_parameters():\n",
        "            if 'weight' in name or 'bias' in name:\n",
        "                # Calculate adaptive sigma for this parameter\n",
        "                if name in grad_magnitudes:\n",
        "                    # Importance: 0 (low gradient) to 1 (max gradient)\n",
        "                    importance = grad_magnitudes[name] / max_grad\n",
        "                    \n",
        "                    # Scale noise: base + extra for important parameters\n",
        "                    # Example with base_sigma=0.01, importance_weight=2.0:\n",
        "                    # - Min importance (0): Ïƒ = 0.01 Ã— (1 + 2Ã—0) = 0.01\n",
        "                    # - Mid importance (0.5): Ïƒ = 0.01 Ã— (1 + 2Ã—0.5) = 0.02\n",
        "                    # - Max importance (1): Ïƒ = 0.01 Ã— (1 + 2Ã—1) = 0.03\n",
        "                    adaptive_sigma = base_sigma * (1 + importance_weight * importance)\n",
        "                else:\n",
        "                    # No gradient (shouldn't happen, but be safe)\n",
        "                    adaptive_sigma = base_sigma\n",
        "                \n",
        "                # Generate and add Gaussian noise with adaptive sigma\n",
        "                noise = torch.randn_like(param) * adaptive_sigma\n",
        "                param.add_(noise)\n",
        "    \n",
        "    return unlearned_model\n",
        "\n",
        "# ============================================================================\n",
        "# EXPERIMENT: Test different base sigma values\n",
        "# ============================================================================\n",
        "# Keep importance_weight=2.0 fixed, vary base_sigma\n",
        "# This tests how the method scales with overall noise level\n",
        "\n",
        "base_sigma_values = [0.001, 0.005, 0.01, 0.05]\n",
        "adaptive_results = []\n",
        "\n",
        "print(\"Testing Adaptive Noise Unlearning...\\n\")\n",
        "\n",
        "for base_sigma in base_sigma_values:\n",
        "    # Apply unlearning with adaptive scaling\n",
        "    unlearned_model = adaptive_noise_unlearning(\n",
        "        model, X_forget_tensor, y_forget_tensor,\n",
        "        base_sigma=base_sigma, \n",
        "        importance_weight=2.0  # Most important params get 3x base noise\n",
        "    )\n",
        "    \n",
        "    # Evaluate on all three sets\n",
        "    retain_acc = evaluate_model(unlearned_model, X_retain_tensor, y_retain_tensor)\n",
        "    forget_acc = evaluate_model(unlearned_model, X_forget_tensor, y_forget_tensor)\n",
        "    test_acc = evaluate_model(unlearned_model, X_test_tensor, y_test_tensor)\n",
        "    \n",
        "    # Calculate parameter distance\n",
        "    param_dist = calculate_parameter_distance(\n",
        "        original_model_state,\n",
        "        get_model_state_dict_copy(unlearned_model)\n",
        "    )\n",
        "    \n",
        "    adaptive_results.append({\n",
        "        'base_sigma': base_sigma,\n",
        "        'retain_acc': retain_acc,\n",
        "        'forget_acc': forget_acc,\n",
        "        'test_acc': test_acc,\n",
        "        'param_dist': param_dist\n",
        "    })\n",
        "    \n",
        "    print(f\"Base Sigma={base_sigma:.3f}: Retain={retain_acc:.2f}%, Forget={forget_acc:.2f}%, Test={test_acc:.2f}%, Dist={param_dist:.4f}\")\n",
        "\n",
        "adaptive_df = pd.DataFrame(adaptive_results)\n",
        "print(\"\\nAdaptive Noise Unlearning Results:\")\n",
        "print(adaptive_df)\n",
        "\n",
        "# EXPECTED BENEFITS OVER UNIFORM NOISE:\n",
        "# - For same forget_acc drop, retain_acc should be higher (better preservation)\n",
        "# - More \"surgical\" unlearning (targeted at forget-specific parameters)\n",
        "# - May achieve good unlearning with lower overall noise levels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Method 4: Layer-wise Noise Injection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# METHOD 4: LAYER-WISE NOISE INJECTION\n",
        "# ============================================================================\n",
        "\n",
        "def layerwise_noise_unlearning(model, layer_sigmas=None):\n",
        "    \"\"\"\n",
        "    Add different noise levels to different layers of the network.\n",
        "    \n",
        "    KEY INSIGHT: Different layers serve different purposes and may need different noise levels!\n",
        "    \n",
        "    NEURAL NETWORK LAYER HIERARCHY:\n",
        "    \n",
        "    Early Layers (close to input):\n",
        "    - Extract low-level features (edges, textures, basic patterns)\n",
        "    - Learn general representations useful across many tasks\n",
        "    - Often shared across different data points\n",
        "    - Less specific to individual samples\n",
        "    \n",
        "    Middle Layers:\n",
        "    - Combine low-level features into higher-level concepts\n",
        "    - Moderately specific to task\n",
        "    \n",
        "    Late Layers (close to output):\n",
        "    - Make final classification decisions\n",
        "    - Most specific to training data\n",
        "    - Directly map features to class labels\n",
        "    - Most responsible for memorization of specific samples\n",
        "    \n",
        "    NOISE STRATEGIES:\n",
        "    \n",
        "    1. INCREASING (Recommended for unlearning):\n",
        "       - Less noise on early layers (preserve general features)\n",
        "       - More noise on late layers (disrupt specific memorization)\n",
        "       - Rationale: Forgetting happens at decision level, not feature level\n",
        "       - Better preserves overall model quality\n",
        "    \n",
        "    2. DECREASING:\n",
        "       - More noise on early layers\n",
        "       - Less noise on late layers\n",
        "       - Rationale: Disrupt feature extraction\n",
        "       - May hurt overall performance significantly\n",
        "    \n",
        "    3. UNIFORM:\n",
        "       - Same noise across all layers (baseline)\n",
        "       - Equivalent to full model Gaussian noise\n",
        "    \n",
        "    4. OUTPUT-HEAVY:\n",
        "       - Minimal noise on all but last layer\n",
        "       - Very high noise only on output layer\n",
        "       - Most targeted approach (only affects final decisions)\n",
        "       - May be most efficient for unlearning\n",
        "    \n",
        "    WHY LAYER-WISE IS USEFUL:\n",
        "    - More control over what gets forgotten\n",
        "    - Can preserve low-level features (useful for retain set)\n",
        "    - Target only decision-making layers\n",
        "    - Research shows: last layers most responsible for memorization\n",
        "    \n",
        "    Args:\n",
        "        model: PyTorch model to apply unlearning to\n",
        "        layer_sigmas: Dictionary mapping layer index â†’ sigma value\n",
        "                     - Index 0 is first linear layer (input â†’ hidden1)\n",
        "                     - Index 1 is second linear layer (hidden1 â†’ hidden2)\n",
        "                     - etc.\n",
        "                     - If None, uses increasing strategy by default\n",
        "    \n",
        "    Returns:\n",
        "        unlearned_model: Model with layer-specific noise applied\n",
        "    \"\"\"\n",
        "    # Create independent copy\n",
        "    unlearned_model = deepcopy(model)\n",
        "    \n",
        "    # ========================================================================\n",
        "    # Extract all linear (fully connected) layers\n",
        "    # ========================================================================\n",
        "    # We only add noise to Linear layers (not BatchNorm, ReLU, etc.)\n",
        "    linear_layers = [(name, module) for name, module in unlearned_model.named_modules() \n",
        "                     if isinstance(module, nn.Linear)]\n",
        "    \n",
        "    num_layers = len(linear_layers)\n",
        "    \n",
        "    # Default strategy: increasing noise for later layers\n",
        "    # Formula: sigma = 0.005 Ã— (layer_index + 1)\n",
        "    # Layer 0: Ïƒ=0.005, Layer 1: Ïƒ=0.010, Layer 2: Ïƒ=0.015, Layer 3: Ïƒ=0.020\n",
        "    if layer_sigmas is None:\n",
        "        layer_sigmas = {i: 0.005 * (i + 1) for i in range(num_layers)}\n",
        "    \n",
        "    # ========================================================================\n",
        "    # Add noise to each layer with its specific sigma\n",
        "    # ========================================================================\n",
        "    with torch.no_grad():\n",
        "        for layer_idx, (name, layer) in enumerate(linear_layers):\n",
        "            # Get sigma for this layer (default to 0.01 if not specified)\n",
        "            sigma = layer_sigmas.get(layer_idx, 0.01)\n",
        "            \n",
        "            # Add noise to weight matrix (most important parameters)\n",
        "            if layer.weight is not None:\n",
        "                noise = torch.randn_like(layer.weight) * sigma\n",
        "                layer.weight.add_(noise)\n",
        "            \n",
        "            # Add noise to bias vector\n",
        "            if layer.bias is not None:\n",
        "                noise = torch.randn_like(layer.bias) * sigma\n",
        "                layer.bias.add_(noise)\n",
        "    \n",
        "    return unlearned_model\n",
        "\n",
        "# ============================================================================\n",
        "# EXPERIMENT: Compare different layer-wise strategies\n",
        "# ============================================================================\n",
        "# Our model has 4 linear layers: [inputâ†’64, 64â†’32, 32â†’16, 16â†’output]\n",
        "# We'll test different hypotheses about which layers to target\n",
        "\n",
        "strategies = [\n",
        "    {\n",
        "        'name': 'Uniform', \n",
        "        'sigmas': {0: 0.01, 1: 0.01, 2: 0.01, 3: 0.01},\n",
        "        # Hypothesis: All layers equally important (baseline)\n",
        "    },\n",
        "    {\n",
        "        'name': 'Increasing', \n",
        "        'sigmas': {0: 0.005, 1: 0.01, 2: 0.015, 3: 0.02},\n",
        "        # Hypothesis: Later layers more responsible for memorization\n",
        "        # Preserve early features, disrupt late decisions\n",
        "    },\n",
        "    {\n",
        "        'name': 'Decreasing', \n",
        "        'sigmas': {0: 0.02, 1: 0.015, 2: 0.01, 3: 0.005},\n",
        "        # Hypothesis: Early layers encode specific patterns\n",
        "        # (Less common approach, included for comparison)\n",
        "    },\n",
        "    {\n",
        "        'name': 'Output-heavy', \n",
        "        'sigmas': {0: 0.005, 1: 0.005, 2: 0.01, 3: 0.03},\n",
        "        # Hypothesis: Only output layer needs disruption\n",
        "        # Most targeted unlearning (minimal collateral damage)\n",
        "    },\n",
        "]\n",
        "\n",
        "layerwise_results = []\n",
        "\n",
        "print(\"Testing Layer-wise Noise Unlearning...\\n\")\n",
        "\n",
        "for strategy in strategies:\n",
        "    # Apply unlearning with this strategy\n",
        "    unlearned_model = layerwise_noise_unlearning(model, layer_sigmas=strategy['sigmas'])\n",
        "    \n",
        "    # Evaluate on all three sets\n",
        "    retain_acc = evaluate_model(unlearned_model, X_retain_tensor, y_retain_tensor)\n",
        "    forget_acc = evaluate_model(unlearned_model, X_forget_tensor, y_forget_tensor)\n",
        "    test_acc = evaluate_model(unlearned_model, X_test_tensor, y_test_tensor)\n",
        "    \n",
        "    # Calculate parameter distance\n",
        "    param_dist = calculate_parameter_distance(\n",
        "        original_model_state,\n",
        "        get_model_state_dict_copy(unlearned_model)\n",
        "    )\n",
        "    \n",
        "    layerwise_results.append({\n",
        "        'strategy': strategy['name'],\n",
        "        'retain_acc': retain_acc,\n",
        "        'forget_acc': forget_acc,\n",
        "        'test_acc': test_acc,\n",
        "        'param_dist': param_dist\n",
        "    })\n",
        "    \n",
        "    print(f\"{strategy['name']:15s}: Retain={retain_acc:.2f}%, Forget={forget_acc:.2f}%, Test={test_acc:.2f}%, Dist={param_dist:.4f}\")\n",
        "\n",
        "layerwise_df = pd.DataFrame(layerwise_results)\n",
        "print(\"\\nLayer-wise Noise Unlearning Results:\")\n",
        "print(layerwise_df)\n",
        "\n",
        "# EXPECTED RESULTS:\n",
        "# - \"Increasing\" should provide good balance (effective forgetting, decent retention)\n",
        "# - \"Output-heavy\" should preserve retain set best (minimal early layer disruption)\n",
        "# - \"Decreasing\" may hurt retain set significantly (disrupts shared features)\n",
        "# - \"Uniform\" serves as baseline for comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Method 5: Gradient-based Noise (Noise Proportional to Gradient)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# METHOD 5: GRADIENT-BASED NOISE (PROPORTIONAL TO GRADIENT MAGNITUDE)\n",
        "# ============================================================================\n",
        "\n",
        "def gradient_based_noise_unlearning(model, X_forget, y_forget, noise_multiplier=0.1):\n",
        "    \"\"\"\n",
        "    Add noise where magnitude is proportional to gradient magnitude on forget set.\n",
        "    \n",
        "    KEY DIFFERENCE FROM ADAPTIVE METHOD:\n",
        "    - Adaptive: Uses gradient to SCALE a fixed noise distribution\n",
        "    - Gradient-based: Uses gradient as DIRECT measure of noise magnitude\n",
        "    \n",
        "    INTUITION:\n",
        "    Think of gradients as \"sensitivity maps\" - they tell us:\n",
        "    \"If I change this parameter by Î”w, the forget set loss changes by gradient Ã— Î”w\"\n",
        "    \n",
        "    So adding noise proportional to gradient means:\n",
        "    - High gradient â†’ Large noise â†’ Big disruption to forget set predictions\n",
        "    - Low gradient â†’ Small noise â†’ Minimal impact (preserves retain set)\n",
        "    \n",
        "    MATHEMATICAL PERSPECTIVE:\n",
        "    \n",
        "    1. Gradient tells us the direction of steepest ascent in loss landscape\n",
        "    2. Large |âˆ‚L/âˆ‚w| means: parameter strongly affects forget set\n",
        "    3. Adding noise ~ |âˆ‚L/âˆ‚w| disrupts exactly these influential parameters\n",
        "    4. This is like \"gradient ascent with noise\" - moving away from memorization\n",
        "    \n",
        "    FORMULA:\n",
        "    noise = randn() Ã— |gradient| Ã— multiplier\n",
        "    \n",
        "    Where:\n",
        "    - randn(): Random Gaussian noise (direction)\n",
        "    - |gradient|: Magnitude of gradient (scale)\n",
        "    - multiplier: Overall strength control\n",
        "    \n",
        "    COMPARISON:\n",
        "    \n",
        "    | Method    | Noise Magnitude                | Per-Parameter |\n",
        "    |-----------|--------------------------------|---------------|\n",
        "    | Gaussian  | Fixed Ïƒ for all                | No            |\n",
        "    | Adaptive  | base_Ïƒ Ã— (1 + w Ã— grad/max)  | Yes           |\n",
        "    | Gradient  | multiplier Ã— |gradient|       | Yes           |\n",
        "    \n",
        "    ADVANTAGES:\n",
        "    - Extremely targeted (noise exactly where it matters)\n",
        "    - No normalization needed (uses raw gradient magnitudes)\n",
        "    - Directly follows gradient landscape\n",
        "    - Can be more aggressive on important parameters\n",
        "    \n",
        "    DISADVANTAGES:\n",
        "    - Very sensitive to multiplier choice\n",
        "    - Gradient magnitudes can vary wildly across parameters\n",
        "    - May be too aggressive if multiplier too high\n",
        "    - Requires forget set access (like adaptive method)\n",
        "    \n",
        "    WHEN TO USE:\n",
        "    - When you want maximum targeting efficiency\n",
        "    - When gradients are well-behaved (not too large or small)\n",
        "    - For experimental comparison with other gradient-based methods\n",
        "    \n",
        "    Args:\n",
        "        model: PyTorch model to apply unlearning to\n",
        "        X_forget: Forget set features (needed to compute gradients)\n",
        "        y_forget: Forget set labels (needed to compute loss)\n",
        "        noise_multiplier: Scaling factor for gradient-based noise\n",
        "                         - 0.1: Conservative (noise magnitude = 10% of gradient)\n",
        "                         - 1.0: Aggressive (noise magnitude = 100% of gradient)\n",
        "                         - > 1.0: Very aggressive (may destabilize model)\n",
        "    \n",
        "    Returns:\n",
        "        unlearned_model: Model with gradient-proportional noise applied\n",
        "    \"\"\"\n",
        "    # Create independent copy\n",
        "    unlearned_model = deepcopy(model)\n",
        "    unlearned_model.eval()\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 1: Compute gradients on forget set\n",
        "    # ========================================================================\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    unlearned_model.zero_grad()\n",
        "    \n",
        "    # Forward and backward pass to get gradients\n",
        "    outputs = unlearned_model(X_forget)\n",
        "    loss = criterion(outputs, y_forget)\n",
        "    loss.backward()\n",
        "    \n",
        "    # Now param.grad contains âˆ‚loss/âˆ‚param for each parameter\n",
        "    \n",
        "    # ========================================================================\n",
        "    # STEP 2: Add noise proportional to gradient magnitude\n",
        "    # ========================================================================\n",
        "    with torch.no_grad():\n",
        "        for name, param in unlearned_model.named_parameters():\n",
        "            if param.grad is not None and ('weight' in name or 'bias' in name):\n",
        "                # Get absolute gradient (magnitude, not direction)\n",
        "                # Shape: same as parameter tensor\n",
        "                grad_magnitude = param.grad.abs()\n",
        "                \n",
        "                # Generate Gaussian noise (random direction)\n",
        "                random_direction = torch.randn_like(param)\n",
        "                \n",
        "                # Scale noise by gradient magnitude\n",
        "                # This means:\n",
        "                # - If gradient[i,j] is large â†’ noise[i,j] is large\n",
        "                # - If gradient[i,j] is small â†’ noise[i,j] is small\n",
        "                noise = random_direction * grad_magnitude * noise_multiplier\n",
        "                \n",
        "                # Add the gradient-scaled noise\n",
        "                param.add_(noise)\n",
        "    \n",
        "    return unlearned_model\n",
        "\n",
        "# ============================================================================\n",
        "# EXPERIMENT: Test different noise multipliers\n",
        "# ============================================================================\n",
        "# The multiplier controls overall aggressiveness of unlearning\n",
        "# We test from conservative (0.05) to very aggressive (1.0)\n",
        "\n",
        "noise_multipliers = [0.05, 0.1, 0.2, 0.5, 1.0]\n",
        "gradient_results = []\n",
        "\n",
        "print(\"Testing Gradient-based Noise Unlearning...\\n\")\n",
        "\n",
        "for multiplier in noise_multipliers:\n",
        "    # Apply unlearning with current multiplier\n",
        "    unlearned_model = gradient_based_noise_unlearning(\n",
        "        model, X_forget_tensor, y_forget_tensor,\n",
        "        noise_multiplier=multiplier\n",
        "    )\n",
        "    \n",
        "    # Evaluate on all three sets\n",
        "    retain_acc = evaluate_model(unlearned_model, X_retain_tensor, y_retain_tensor)\n",
        "    forget_acc = evaluate_model(unlearned_model, X_forget_tensor, y_forget_tensor)\n",
        "    test_acc = evaluate_model(unlearned_model, X_test_tensor, y_test_tensor)\n",
        "    \n",
        "    # Calculate parameter distance\n",
        "    param_dist = calculate_parameter_distance(\n",
        "        original_model_state,\n",
        "        get_model_state_dict_copy(unlearned_model)\n",
        "    )\n",
        "    \n",
        "    gradient_results.append({\n",
        "        'multiplier': multiplier,\n",
        "        'retain_acc': retain_acc,\n",
        "        'forget_acc': forget_acc,\n",
        "        'test_acc': test_acc,\n",
        "        'param_dist': param_dist\n",
        "    })\n",
        "    \n",
        "    print(f\"Multiplier={multiplier:.2f}: Retain={retain_acc:.2f}%, Forget={forget_acc:.2f}%, Test={test_acc:.2f}%, Dist={param_dist:.4f}\")\n",
        "\n",
        "gradient_df = pd.DataFrame(gradient_results)\n",
        "print(\"\\nGradient-based Noise Unlearning Results:\")\n",
        "print(gradient_df)\n",
        "\n",
        "# EXPECTED BEHAVIOR:\n",
        "# - Should be very effective at forgetting (targets exactly the right parameters)\n",
        "# - May show sharp drop in forget_acc even at low multipliers\n",
        "# - Retain set may be preserved better than uniform noise (targeted approach)\n",
        "# - Parameter distance may be large (gradient magnitudes can be significant)\n",
        "# - Higher multipliers may cause instability or over-forgetting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Comparison and Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison plots\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "fig.suptitle('Comparison of Noise-Based Machine Unlearning Methods', fontsize=16, y=1.00)\n",
        "\n",
        "# 1. Gaussian Noise\n",
        "ax = axes[0, 0]\n",
        "ax.plot(gaussian_df['sigma'], gaussian_df['retain_acc'], 'o-', label='Retain', linewidth=2)\n",
        "ax.plot(gaussian_df['sigma'], gaussian_df['forget_acc'], 's-', label='Forget', linewidth=2)\n",
        "ax.plot(gaussian_df['sigma'], gaussian_df['test_acc'], '^-', label='Test', linewidth=2)\n",
        "ax.set_xlabel('Sigma')\n",
        "ax.set_ylabel('Accuracy (%)')\n",
        "ax.set_title('Gaussian Noise')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Laplacian Noise\n",
        "ax = axes[0, 1]\n",
        "ax.plot(laplacian_df['scale'], laplacian_df['retain_acc'], 'o-', label='Retain', linewidth=2)\n",
        "ax.plot(laplacian_df['scale'], laplacian_df['forget_acc'], 's-', label='Forget', linewidth=2)\n",
        "ax.plot(laplacian_df['scale'], laplacian_df['test_acc'], '^-', label='Test', linewidth=2)\n",
        "ax.set_xlabel('Scale')\n",
        "ax.set_ylabel('Accuracy (%)')\n",
        "ax.set_title('Laplacian Noise')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Adaptive Noise\n",
        "ax = axes[0, 2]\n",
        "ax.plot(adaptive_df['base_sigma'], adaptive_df['retain_acc'], 'o-', label='Retain', linewidth=2)\n",
        "ax.plot(adaptive_df['base_sigma'], adaptive_df['forget_acc'], 's-', label='Forget', linewidth=2)\n",
        "ax.plot(adaptive_df['base_sigma'], adaptive_df['test_acc'], '^-', label='Test', linewidth=2)\n",
        "ax.set_xlabel('Base Sigma')\n",
        "ax.set_ylabel('Accuracy (%)')\n",
        "ax.set_title('Adaptive Noise')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Layer-wise Noise\n",
        "ax = axes[1, 0]\n",
        "x_pos = np.arange(len(layerwise_df))\n",
        "width = 0.25\n",
        "ax.bar(x_pos - width, layerwise_df['retain_acc'], width, label='Retain')\n",
        "ax.bar(x_pos, layerwise_df['forget_acc'], width, label='Forget')\n",
        "ax.bar(x_pos + width, layerwise_df['test_acc'], width, label='Test')\n",
        "ax.set_xlabel('Strategy')\n",
        "ax.set_ylabel('Accuracy (%)')\n",
        "ax.set_title('Layer-wise Noise')\n",
        "ax.set_xticks(x_pos)\n",
        "ax.set_xticklabels(layerwise_df['strategy'], rotation=45, ha='right')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 5. Gradient-based Noise\n",
        "ax = axes[1, 1]\n",
        "ax.plot(gradient_df['multiplier'], gradient_df['retain_acc'], 'o-', label='Retain', linewidth=2)\n",
        "ax.plot(gradient_df['multiplier'], gradient_df['forget_acc'], 's-', label='Forget', linewidth=2)\n",
        "ax.plot(gradient_df['multiplier'], gradient_df['test_acc'], '^-', label='Test', linewidth=2)\n",
        "ax.set_xlabel('Noise Multiplier')\n",
        "ax.set_ylabel('Accuracy (%)')\n",
        "ax.set_title('Gradient-based Noise')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 6. Parameter Distance Comparison\n",
        "ax = axes[1, 2]\n",
        "methods = ['Gaussian\\n(Ïƒ=0.01)', 'Laplacian\\n(s=0.01)', 'Adaptive\\n(Ïƒ=0.01)', \n",
        "           'Layer-wise\\n(Increasing)', 'Gradient\\n(m=0.1)']\n",
        "distances = [\n",
        "    gaussian_df[gaussian_df['sigma'] == 0.01]['param_dist'].values[0],\n",
        "    laplacian_df[laplacian_df['scale'] == 0.01]['param_dist'].values[0],\n",
        "    adaptive_df[adaptive_df['base_sigma'] == 0.01]['param_dist'].values[0],\n",
        "    layerwise_df[layerwise_df['strategy'] == 'Increasing']['param_dist'].values[0],\n",
        "    gradient_df[gradient_df['multiplier'] == 0.1]['param_dist'].values[0]\n",
        "]\n",
        "ax.bar(methods, distances, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])\n",
        "ax.set_ylabel('Parameter Distance')\n",
        "ax.set_title('Parameter Distance from Original')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary Table and Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# COMPREHENSIVE SUMMARY: Compare All Methods\n",
        "# ============================================================================\n",
        "# Select representative configurations from each method for fair comparison\n",
        "# We choose middle-range parameters (not too aggressive, not too conservative)\n",
        "\n",
        "# Create summary table with best configuration from each method\n",
        "summary_data = [\n",
        "    {\n",
        "        'Method': 'Original Model',\n",
        "        'Retain Acc': evaluate_model(model, X_retain_tensor, y_retain_tensor),\n",
        "        'Forget Acc': evaluate_model(model, X_forget_tensor, y_forget_tensor),\n",
        "        'Test Acc': evaluate_model(model, X_test_tensor, y_test_tensor),\n",
        "        'Param Dist': 0.0,  # No change from original\n",
        "        'Config': 'N/A'\n",
        "    },\n",
        "    {\n",
        "        'Method': 'Gaussian Noise',\n",
        "        'Retain Acc': gaussian_df.iloc[2]['retain_acc'],  # iloc[2] = Ïƒ=0.01 (moderate)\n",
        "        'Forget Acc': gaussian_df.iloc[2]['forget_acc'],\n",
        "        'Test Acc': gaussian_df.iloc[2]['test_acc'],\n",
        "        'Param Dist': gaussian_df.iloc[2]['param_dist'],\n",
        "        'Config': f\"Ïƒ={gaussian_df.iloc[2]['sigma']}\"\n",
        "    },\n",
        "    {\n",
        "        'Method': 'Laplacian Noise',\n",
        "        'Retain Acc': laplacian_df.iloc[2]['retain_acc'],  # iloc[2] = scale=0.01 (moderate)\n",
        "        'Forget Acc': laplacian_df.iloc[2]['forget_acc'],\n",
        "        'Test Acc': laplacian_df.iloc[2]['test_acc'],\n",
        "        'Param Dist': laplacian_df.iloc[2]['param_dist'],\n",
        "        'Config': f\"scale={laplacian_df.iloc[2]['scale']}\"\n",
        "    },\n",
        "    {\n",
        "        'Method': 'Adaptive Noise',\n",
        "        'Retain Acc': adaptive_df.iloc[2]['retain_acc'],  # iloc[2] = base_Ïƒ=0.01\n",
        "        'Forget Acc': adaptive_df.iloc[2]['forget_acc'],\n",
        "        'Test Acc': adaptive_df.iloc[2]['test_acc'],\n",
        "        'Param Dist': adaptive_df.iloc[2]['param_dist'],\n",
        "        'Config': f\"base_Ïƒ={adaptive_df.iloc[2]['base_sigma']}\"\n",
        "    },\n",
        "    {\n",
        "        'Method': 'Layer-wise Noise',\n",
        "        'Retain Acc': layerwise_df.iloc[1]['retain_acc'],  # iloc[1] = \"Increasing\" strategy\n",
        "        'Forget Acc': layerwise_df.iloc[1]['forget_acc'],\n",
        "        'Test Acc': layerwise_df.iloc[1]['test_acc'],\n",
        "        'Param Dist': layerwise_df.iloc[1]['param_dist'],\n",
        "        'Config': layerwise_df.iloc[1]['strategy']\n",
        "    },\n",
        "    {\n",
        "        'Method': 'Gradient-based Noise',\n",
        "        'Retain Acc': gradient_df.iloc[1]['retain_acc'],  # iloc[1] = multiplier=0.1\n",
        "        'Forget Acc': gradient_df.iloc[1]['forget_acc'],\n",
        "        'Test Acc': gradient_df.iloc[1]['test_acc'],\n",
        "        'Param Dist': gradient_df.iloc[1]['param_dist'],\n",
        "        'Config': f\"mult={gradient_df.iloc[1]['multiplier']}\"\n",
        "    }\n",
        "]\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "\n",
        "# Display main summary table\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SUMMARY: Noise-Based Machine Unlearning Methods\")\n",
        "print(\"=\"*80)\n",
        "print(summary_df.to_string(index=False))\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ============================================================================\n",
        "# UNLEARNING EFFECTIVENESS METRICS\n",
        "# ============================================================================\n",
        "# Calculate and display key metrics that measure unlearning quality\n",
        "#\n",
        "# METRIC DEFINITIONS:\n",
        "# 1. Forget Drop: How much forget set accuracy decreased (HIGHER is better)\n",
        "#    - Measures effectiveness of forgetting\n",
        "#    - Target: Should decrease significantly\n",
        "#\n",
        "# 2. Retain Drop: How much retain set accuracy decreased (LOWER is better)\n",
        "#    - Measures collateral damage to retained knowledge\n",
        "#    - Target: Should stay close to 0\n",
        "#\n",
        "# 3. Quality Score: Forget Drop - Retain Drop (HIGHER is better)\n",
        "#    - Balances forgetting effectiveness vs. retention preservation\n",
        "#    - Positive score: More forgetting than damage (good!)\n",
        "#    - Negative score: More damage than forgetting (bad!)\n",
        "#    - Higher score = better unlearning method\n",
        "\n",
        "print(\"\\nUnlearning Effectiveness Metrics:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"{'Method':<25s} | {'Forget Drop':>12s} | {'Retain Drop':>12s} | {'Quality Score':>14s}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "original_forget_acc = summary_df.iloc[0]['Forget Acc']\n",
        "original_retain_acc = summary_df.iloc[0]['Retain Acc']\n",
        "\n",
        "for idx, row in summary_df.iterrows():\n",
        "    if idx == 0:  # Skip original model\n",
        "        continue\n",
        "    \n",
        "    # Calculate drops (positive values mean decrease in accuracy)\n",
        "    forget_drop = original_forget_acc - row['Forget Acc']  # Want HIGH\n",
        "    retain_drop = original_retain_acc - row['Retain Acc']  # Want LOW\n",
        "    \n",
        "    # Quality score: prioritize forgetting while minimizing retention loss\n",
        "    quality_score = forget_drop - retain_drop  # Want HIGH\n",
        "    \n",
        "    print(f\"{row['Method']:25s} | {forget_drop:11.2f}% | {retain_drop:11.2f}% | {quality_score:13.2f}\")\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print(\"\\nINTERPRETATION GUIDE:\")\n",
        "print(\"  â€¢ Forget Drop: How much the model forgot (higher = better forgetting)\")\n",
        "print(\"  â€¢ Retain Drop: How much retained knowledge was lost (lower = better)\")\n",
        "print(\"  â€¢ Quality Score: Overall unlearning quality (higher = better trade-off)\")\n",
        "print(\"  â€¢ Best method: High forget drop + low retain drop = high quality score\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Retrain from Scratch (Gold Standard Baseline)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# GOLD STANDARD BASELINE: RETRAIN FROM SCRATCH\n",
        "# ============================================================================\n",
        "# This is the \"perfect\" unlearning method - the theoretical ideal we compare against\n",
        "#\n",
        "# WHY RETRAIN FROM SCRATCH?\n",
        "# - It's what we would do if we could afford the computational cost\n",
        "# - Model trained ONLY on retain set has truly \"never seen\" the forget set\n",
        "# - No information leakage from forget set (perfect privacy)\n",
        "# - But: Expensive (requires full retraining) and not always feasible\n",
        "#\n",
        "# Machine unlearning research goal:\n",
        "# Find methods that approximate retrain-from-scratch but are much faster\n",
        "\n",
        "# Create a fresh model with same architecture\n",
        "# IMPORTANT: Completely new initialization (different random weights)\n",
        "retrain_model = TabularClassifier(\n",
        "    input_size=n_features,\n",
        "    hidden_sizes=[64, 32, 16],  # Same architecture as original\n",
        "    num_classes=2,\n",
        "    dropout=0.3\n",
        ").to(device)\n",
        "\n",
        "print(\"Training model from scratch on retain set only...\")\n",
        "# Train ONLY on retain set (forget set is never seen)\n",
        "# This simulates: \"What if we never had the forget set data in the first place?\"\n",
        "retrain_losses = train_model(\n",
        "    retrain_model, \n",
        "    X_retain_tensor,  # Only retain set (NO forget set)\n",
        "    y_retain_tensor, \n",
        "    epochs=100,  # Same training duration as original\n",
        "    lr=0.001,\n",
        "    verbose=False  # Suppress output for cleaner notebook\n",
        ")\n",
        "\n",
        "# ========================================================================\n",
        "# EVALUATE RETRAINED MODEL\n",
        "# ========================================================================\n",
        "# This gives us the \"ideal\" performance to target with unlearning methods\n",
        "\n",
        "# RETAIN SET: Should be similar to original (still learned this data)\n",
        "retrain_retain_acc = evaluate_model(retrain_model, X_retain_tensor, y_retain_tensor)\n",
        "\n",
        "# FORGET SET: Should be LOWER than original (never trained on this)\n",
        "# This is the \"target\" forget accuracy for unlearning methods\n",
        "# Represents performance of a model that truly never saw this data\n",
        "retrain_forget_acc = evaluate_model(retrain_model, X_forget_tensor, y_forget_tensor)\n",
        "\n",
        "# TEST SET: May be slightly lower than original (less training data)\n",
        "# Trade-off: perfect unlearning but slightly less generalization\n",
        "retrain_test_acc = evaluate_model(retrain_model, X_test_tensor, y_test_tensor)\n",
        "\n",
        "print(\"\\n=== Retrained Model (Gold Standard) ===\")\n",
        "print(f\"Retain Set Accuracy: {retrain_retain_acc:.2f}%\")\n",
        "print(f\"Forget Set Accuracy: {retrain_forget_acc:.2f}%\")  # Target for unlearning methods\n",
        "print(f\"Test Set Accuracy: {retrain_test_acc:.2f}%\")\n",
        "\n",
        "# ========================================================================\n",
        "# COMPARISON TABLE\n",
        "# ========================================================================\n",
        "# Compare all unlearning methods against the gold standard\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Comparison with Gold Standard (Retrain from Scratch):\")\n",
        "print(\"=\"*80)\n",
        "print(f\"{'Method':<25s} | {'Retain Acc':>10s} | {'Forget Acc':>10s} | {'Test Acc':>10s}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Gold standard first\n",
        "print(f\"{'Retrain (Gold Standard)':<25s} | {retrain_retain_acc:>9.2f}% | {retrain_forget_acc:>9.2f}% | {retrain_test_acc:>9.2f}%\")\n",
        "\n",
        "# Then all noise-based methods\n",
        "for idx, row in summary_df.iterrows():\n",
        "    if idx == 0:  # Skip original model\n",
        "        continue\n",
        "    print(f\"{row['Method']:<25s} | {row['Retain Acc']:>9.2f}% | {row['Forget Acc']:>9.2f}% | {row['Test Acc']:>9.2f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"INTERPRETATION:\")\n",
        "print(\"-\"*80)\n",
        "print(\"IDEAL UNLEARNING METHOD should:\")\n",
        "print(f\"  - Forget Acc â‰ˆ {retrain_forget_acc:.2f}% (match gold standard)\")\n",
        "print(f\"  - Retain Acc â‰ˆ {retrain_retain_acc:.2f}% (maintain performance)\")\n",
        "print(f\"  - Test Acc â‰ˆ {retrain_test_acc:.2f}% (preserve generalization)\")\n",
        "print(\"\\nMethods closer to these values are better approximations of true unlearning.\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Unlearning Effectiveness Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a comprehensive visualization of unlearning effectiveness\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "fig.suptitle('Unlearning Effectiveness Analysis', fontsize=16)\n",
        "\n",
        "# 1. Forget vs Retain Accuracy Trade-off\n",
        "ax = axes[0, 0]\n",
        "for idx, row in summary_df.iterrows():\n",
        "    if idx == 0:\n",
        "        ax.scatter(row['Retain Acc'], row['Forget Acc'], s=200, marker='*', \n",
        "                  c='red', label='Original', zorder=5, edgecolors='black', linewidth=2)\n",
        "    else:\n",
        "        ax.scatter(row['Retain Acc'], row['Forget Acc'], s=100, \n",
        "                  label=row['Method'], alpha=0.7, edgecolors='black', linewidth=1)\n",
        "\n",
        "ax.scatter(retrain_retain_acc, retrain_forget_acc, s=200, marker='D', \n",
        "          c='green', label='Retrain (Gold)', zorder=5, edgecolors='black', linewidth=2)\n",
        "ax.set_xlabel('Retain Set Accuracy (%)', fontsize=11)\n",
        "ax.set_ylabel('Forget Set Accuracy (%)', fontsize=11)\n",
        "ax.set_title('Forget vs Retain Accuracy Trade-off\\n(Lower-Right is Better)', fontsize=12)\n",
        "ax.legend(fontsize=8, loc='best')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Unlearning Quality Score\n",
        "ax = axes[0, 1]\n",
        "methods_list = []\n",
        "quality_scores = []\n",
        "colors_list = []\n",
        "\n",
        "for idx, row in summary_df.iterrows():\n",
        "    if idx == 0:\n",
        "        continue\n",
        "    forget_drop = summary_df.iloc[0]['Forget Acc'] - row['Forget Acc']\n",
        "    retain_drop = summary_df.iloc[0]['Retain Acc'] - row['Retain Acc']\n",
        "    quality = forget_drop - retain_drop\n",
        "    methods_list.append(row['Method'])\n",
        "    quality_scores.append(quality)\n",
        "    colors_list.append('#2ca02c' if quality > 0 else '#d62728')\n",
        "\n",
        "bars = ax.barh(methods_list, quality_scores, color=colors_list, alpha=0.7, edgecolor='black')\n",
        "ax.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
        "ax.set_xlabel('Quality Score (Forget Drop - Retain Drop)', fontsize=11)\n",
        "ax.set_title('Unlearning Quality Score\\n(Higher is Better)', fontsize=12)\n",
        "ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# 3. Accuracy Changes from Original\n",
        "ax = axes[1, 0]\n",
        "x_pos = np.arange(len(methods_list))\n",
        "width = 0.35\n",
        "\n",
        "forget_drops = [summary_df.iloc[0]['Forget Acc'] - summary_df.iloc[i+1]['Forget Acc'] for i in range(len(methods_list))]\n",
        "retain_drops = [summary_df.iloc[0]['Retain Acc'] - summary_df.iloc[i+1]['Retain Acc'] for i in range(len(methods_list))]\n",
        "\n",
        "ax.bar(x_pos - width/2, forget_drops, width, label='Forget Acc Drop', color='#ff7f0e', alpha=0.7, edgecolor='black')\n",
        "ax.bar(x_pos + width/2, retain_drops, width, label='Retain Acc Drop', color='#1f77b4', alpha=0.7, edgecolor='black')\n",
        "\n",
        "ax.set_ylabel('Accuracy Drop (%)', fontsize=11)\n",
        "ax.set_title('Accuracy Changes from Original Model\\n(Higher Forget Drop is Better)', fontsize=12)\n",
        "ax.set_xticks(x_pos)\n",
        "ax.set_xticklabels(methods_list, rotation=45, ha='right', fontsize=9)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "\n",
        "# 4. Parameter Distance vs Unlearning Effectiveness\n",
        "ax = axes[1, 1]\n",
        "param_dists = [summary_df.iloc[i+1]['Param Dist'] for i in range(len(methods_list))]\n",
        "forget_accs = [summary_df.iloc[i+1]['Forget Acc'] for i in range(len(methods_list))]\n",
        "\n",
        "scatter = ax.scatter(param_dists, forget_accs, s=100, c=quality_scores, \n",
        "                    cmap='RdYlGn', alpha=0.7, edgecolors='black', linewidth=1)\n",
        "\n",
        "# Add method labels\n",
        "for i, method in enumerate(methods_list):\n",
        "    ax.annotate(method.split()[0], (param_dists[i], forget_accs[i]), \n",
        "               fontsize=8, ha='center', va='bottom')\n",
        "\n",
        "ax.set_xlabel('Parameter Distance from Original', fontsize=11)\n",
        "ax.set_ylabel('Forget Set Accuracy (%)', fontsize=11)\n",
        "ax.set_title('Parameter Distance vs Forget Accuracy\\n(Lower Forget Acc is Better)', fontsize=12)\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.colorbar(scatter, ax=ax, label='Quality Score')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Key Findings and Recommendations\n",
        "\n",
        "### Understanding the Results:\n",
        "\n",
        "1. **Forget Set Accuracy**: Should decrease after unlearning (model \"forgets\" this data)\n",
        "2. **Retain Set Accuracy**: Should remain high (model maintains knowledge of retained data)\n",
        "3. **Test Set Accuracy**: Overall model performance indicator\n",
        "4. **Parameter Distance**: How much the model changed from original\n",
        "\n",
        "### Method Characteristics:\n",
        "\n",
        "- **Gaussian Noise**: Simple, uniform perturbation across all parameters. Easy to implement and tune.\n",
        "- **Laplacian Noise**: Better for differential privacy guarantees due to heavier tails. Provides formal privacy bounds.\n",
        "- **Adaptive Noise**: Targets important parameters (high gradients on forget set) more aggressively. Better trade-off between forgetting and retaining.\n",
        "- **Layer-wise Noise**: Allows fine-grained control per layer. Can focus noise on output layers that directly influence predictions.\n",
        "- **Gradient-based Noise**: Focuses on parameters most responsible for forget set. Noise magnitude scales with parameter importance.\n",
        "\n",
        "### Trade-offs:\n",
        "\n",
        "- **More noise** â†’ Better forgetting but worse retain/test performance\n",
        "- **Less noise** â†’ Better retain/test performance but less effective forgetting\n",
        "- **Adaptive methods** â†’ Better balance between forgetting and retaining\n",
        "- **Computational cost**: Gradient-based and adaptive methods require forward/backward pass on forget set\n",
        "\n",
        "### Best Practices:\n",
        "\n",
        "1. **Start small**: Begin with small noise levels and increase gradually\n",
        "2. **Monitor both sets**: Track both forget and retain set performance\n",
        "3. **Use adaptive methods**: For better trade-offs, consider gradient-based or adaptive noise\n",
        "4. **Compare to baseline**: Always compare against retrain-from-scratch baseline\n",
        "5. **Consider privacy**: Use Laplacian noise if differential privacy guarantees are needed\n",
        "6. **Layer targeting**: Focus noise on later layers (closer to output) for more targeted forgetting\n",
        "\n",
        "### When to Use Each Method:\n",
        "\n",
        "- **Gaussian Noise**: Quick experiments, baseline comparisons\n",
        "- **Laplacian Noise**: When differential privacy is required\n",
        "- **Adaptive Noise**: When you want to minimize impact on retained data\n",
        "- **Layer-wise Noise**: When you know which layers are most important\n",
        "- **Gradient-based Noise**: When you want targeted, efficient unlearning\n",
        "\n",
        "### Limitations:\n",
        "\n",
        "1. Noise-based methods are **approximate** - they don't guarantee complete removal of information\n",
        "2. May require careful tuning of noise levels\n",
        "3. Can degrade overall model performance\n",
        "4. No formal guarantees about what information is removed\n",
        "5. May not work well for very small forget sets or highly correlated data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
